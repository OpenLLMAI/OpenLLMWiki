<div style="font-size: 1.5rem;">
  <a href="./README.md">ä¸­æ–‡</a> |
  <a href="./readme_en.md">English</a>
</div>



</br>

<h1 align="center">ChatPiXiu</h1>
<div align="center">
  <a href="https://github.com/catqaq/ChatPiXiu">
    <img src="docs/imgs/pixiu.jpg" alt="Logo" height="210">
  </a>


  <p align="center">
    <h3>ChatPiXiu: Eat every ChatGPT - Output your own chatbot.</h3>
      <a href="https://github.com/catqaq/ChatPiXiu/graphs/contributors">
        <img alt="GitHub Contributors" src="https://img.shields.io/github/contributors/catqaq/ChatPiXiu" />
      </a>
      <a href="https://github.com/catqaq/ChatPiXiu/issues">
        <img alt="Issues" src="https://img.shields.io/github/issues/catqaq/ChatPiXiu?color=0088ff" />
      </a>
      <a href="https://github.com/catqaq/ChatPiXiu/discussions">
        <img alt="Issues" src="https://img.shields.io/github/discussions/catqaq/ChatPiXiu?color=0088ff" />
      </a>
      <a href="https://github.com/catqaq/ChatPiXiu/pulls">
        <img alt="GitHub pull requests" src="https://img.shields.io/github/issues-pr/catqaq/ChatPiXiu?color=0088ff" />
      <a href="https://github.com/catqaq/ChatPiXiu/stargazers">
        <img alt="GitHub stars" src="https://img.shields.io/github/stars/catqaq/ChatPiXiu?color=ccf" />
      </a>
      <br/>
      <em>å¼€æºChatGPT / å…¨é¢ / è½»é‡ / æ˜“ç”¨ </em>
      <br/>
      <a href="https://zhuanlan.zhihu.com/p/622065348/"><strong>æ–‡ç« è§£è¯»</strong></a>
        Â·
      <a href="https://zhuanlan.zhihu.com/p/622065348"><strong>è§†é¢‘è§£è¯»</strong></a>
    </p>






 </p>
</div>

> **ä»£ç å¼€æºï¼Œå¤§å®¶æ”¾å¿ƒä½¿ç”¨ï¼Œæ¬¢è¿è´¡çŒ®! æ³¨æ„ï¼šæ¨¡å‹çš„licenseå–å†³äºæ¨¡å‹æä¾›æ–¹**


- [ğŸ’¥æœ€æ–°è®¯æ¯](#æœ€æ–°è®¯æ¯)
- [ğŸ’«OpenNLPè®¡åˆ’](#OpenNLPè®¡åˆ’)
- [ğŸ’«OpenChat-PiXiu](#ChatPiXiué¡¹ç›®)
- [ğŸŒŸå¼€æºChatGPTè°ƒç ”](#å¼€æºChatGPTè°ƒç ”)
- [â›ï¸ä½¿ç”¨æ­¥éª¤](#ä½¿ç”¨æ­¥éª¤)
- [ğŸ“„è¿è¡Œç¤ºä¾‹](#è¿è¡Œç¤ºä¾‹)
- [ğŸ“„ç»“æœå±•ç¤º](#ç»“æœå±•ç¤º)
- [ğŸ› ï¸å¸¸è§æŠ¥é”™](#å¸¸è§æŠ¥é”™)
- [ğŸ’å‚è€ƒèµ„æ–™&è‡´è°¢](#å‚è€ƒèµ„æ–™&è‡´è°¢)
- [ğŸŒŸèµåŠ©æˆ‘ä»¬](#èµåŠ©æˆ‘ä»¬)
- [ğŸŒˆStarchart](#Starchart)
- [ğŸ†Contributors](#Contributors)




## æœ€æ–°è®¯æ¯

- 2023/04/14ï¼šChatPiXiué¡¹ç›®æ­£å¼å¯åŠ¨ï¼š
  - å¼€æºChatGPTå¹³æ›¿è°ƒç ”æ±‡æ€»
  - 

## OpenNLPè®¡åˆ’

æˆ‘ä»¬æ˜¯è°ï¼Ÿ

æˆ‘ä»¬æ˜¯**ç¾¡é±¼æ™ºèƒ½**ã€xianyu.aiã€‘ï¼Œä¸»è¦æˆå‘˜æ˜¯ä¸€ç¾¤æ¥è‡ªè€å’Œå±±ä¸‹ã€è¥¿æ¹–è¾¹ä¸Šçš„å’¸é±¼ä»¬ï¼Œå¡˜ä¸»å«ä½œç¾¡é±¼ï¼Œæƒ³åœ¨LLMsæ—¶ä»£åšç‚¹æœ‰æ„ä¹‰çš„äº‹ï¼æˆ‘ä»¬çš„å£å·æ˜¯ï¼š**åšOpenNLPå’ŒOpenXï¼å¸Œæœ›åœ¨CloseAIå·æ­»æˆ‘ä»¬ä¹‹å‰é€€å‡ºæ±Ÿæ¹–ï¼**

ä¹Ÿè®¸æœ‰ä¸€å¤©ï¼Œç­‰åˆ°GPT-Xå‘å¸ƒçš„æ—¶å€™ï¼Œæœ‰äººä¼šè¯´NLPä¸å­˜åœ¨äº†ï¼Œä½†æ˜¯æˆ‘ä»¬æƒ³è¯æ˜æœ‰äººæ›¾ç»æ¥è¿‡ã€çƒ­çˆ±è¿‡ï¼åœ¨ä»¥ChatGPT/GPT4ä¸ºä»£è¡¨çš„LLMsæ—¶ä»£ï¼Œåœ¨è¢«CloseAIå·æ­»ä¹‹å‰ï¼Œæˆ‘ä»¬å‘èµ·äº†OpenNLPè®¡åˆ’ï¼Œå®—æ—¨æ˜¯OpenNLP for everyone! 

- ã€P0ã€‘[OpenTextClassification](https://github.com/catqaq/OpenTextClassification)ï¼šæ‰“é€ ä¸€æµçš„æ–‡æœ¬åˆ†ç±»é¡¹ç›®ï¼Œå·²å¼€æº
  - ç»¼è¿°ï¼šdone
  - å¼€æºé¡¹ç›®ï¼šdone
  - papersè§£è¯»ï¼šdoing
  - ç‚¼ä¸¹æœ¯ï¼šdone
- ã€P0ã€‘OpenSEï¼šå¥åµŒå…¥ï¼Œè‡ªç„¶è¯­è¨€å¤„ç†çš„æ ¸å¿ƒé—®é¢˜ä¹‹ä¸€ï¼Œdoing
- ã€P0ã€‘[ChatPiXiu](https://github.com/catqaq/ChatPiXiu)ï¼šChatGPTå¼€æºå¹³æ›¿åŠé¢†åŸŸé€‚åº”ï¼Œdoing
- ã€P1ã€‘OpenLLMsï¼šå¤§è¯­è¨€æ¨¡å‹ï¼Œdoing
- ã€P2ã€‘OpenTextTaggerï¼šæ–‡æœ¬æ ‡æ³¨ï¼Œåˆ†è¯ã€NERã€è¯æ€§æ ‡æ³¨ç­‰
- OpenXï¼šä»»é‡è€Œé“è¿œ

## ChatPiXiué¡¹ç›®

ChatPiXiué¡¹ç›®ä¸ºOpenNLPè®¡åˆ’çš„ç¬¬2ä¸ªæ­£å¼çš„å¼€æºé¡¹ç›®ï¼Œæ—¨åœ¨Open ChatGPT for everyoneï¼åœ¨ä»¥ChatGPT/GPT4ä¸ºä»£è¡¨çš„LLMsæ—¶ä»£ï¼Œåœ¨è¢«OpenAIå·æ­»ä¹‹å‰ï¼Œåšä¸€ç‚¹æœ‰æ„ä¹‰çš„äº‹æƒ…ï¼æœªæ¥æœ‰ä¸€å¤©ï¼Œç­‰åˆ°GPT-Xå‘å¸ƒçš„æ—¶å€™ï¼Œæˆ–è®¸æœ‰äººä¼šè¯´NLPä¸å­˜åœ¨äº†ï¼Œä½†æ˜¯æˆ‘ä»¬æƒ³è¯æ˜æœ‰äººæ›¾æ¥è¿‡ï¼

### 1.å¼€å‘è®¡åˆ’

æœ¬é¡¹ç›®çš„å¼€å‘å®—æ—¨ï¼Œæ‰“é€ å…¨é¢ä¸”å®ç”¨çš„ChatGPTæ¨¡å‹åº“å’Œæ–‡æ¡£åº“ã€‚**Eat every ChatGPT - Output your own chatbot!**

ç›®å‰æˆ‘ä»¬æ­£åœ¨å¯åŠ¨V1ç‰ˆæœ¬çš„å¼€å‘ï¼Œæ•´ä½“çš„å¼€å‘è®¡åˆ’å¦‚ä¸‹ï¼Œä¸»è¦åŒ…æ‹¬äº†æ–‡æ¡£å’Œä»£ç ä¸¤ç±»ä»»åŠ¡ï¼Œæ•°æ®çš„éƒ¨åˆ†æˆ‘ä»¬æš‚æ—¶å°†å…¶åˆ†æ•£åˆ°äº†å„ä¸ªå­ä»»åŠ¡ä¸­ã€‚

**V1ç‰ˆæœ¬ï¼šèµ„æ–™è°ƒç ”+é€šç”¨æœ€å°å®ç°+é¢†åŸŸ/ä»»åŠ¡é€‚é…**

#### 1.1 æ–‡æ¡£åˆ†æ”¯

æ–‡æ¡£åˆ†æ”¯ä¸»è¦è´Ÿè´£é¡¹ç›®æ–‡æ¡£çš„å»ºè®¾ï¼ŒåŒ…æ‹¬é€šç”¨æŠ€æœ¯æ–‡æ¡£å’Œé¡¹ç›®ç›¸å…³æ–‡æ¡£ã€‚

**dev_for_docs**ï¼šæ–‡æ¡£åˆ†æ”¯ï¼Œä¸»è¦è´Ÿè´£èµ„æ–™è°ƒç ”ï¼ˆç®—åŠ›æœ‰é™ï¼Œæœ‰è°ƒæŸ¥æ‰æœ‰è®­ç»ƒæƒï¼‰ï¼š

1. ã€P0ã€‘å¼€æºChatGPTè°ƒç ”ï¼šæŒç»­æ›´æ–°ï¼Œdoing
2. ã€P0ã€‘è®­ç»ƒæŠ€æœ¯è°ƒç ”ï¼šæŒç»­æ›´æ–°ï¼Œdoing
3. ã€P0ã€‘æ•°æ®è°ƒç ”ï¼šdoing
4. ã€P1ã€‘éƒ¨ç½²æŠ€æœ¯è°ƒç ”ï¼šTODO
5. ã€P2ã€‘åŸºç¡€æ¨¡å‹è°ƒç ”ï¼šç›®å‰ä»¥LLaMAå’ŒGLMä¸ºä¸»ï¼Œdoing
6. ã€P3ã€‘æŠ€æœ¯è§£è¯»/æ•™ç¨‹ï¼šdoing



#### 1.2 ä»£ç åˆ†æ”¯

ä»£ç åˆ†æ”¯ï¼Œè´Ÿè´£å…·ä½“çš„å¼€å‘å·¥ä½œï¼ŒåŒ…æ‹¬æ•°æ®å¤„ç†ã€ç®—æ³•å¼€å‘ã€ç®—æ³•è¯„æµ‹ç­‰ï¼Œåˆ†æˆé€šç”¨æœ€å°å®ç°å’Œé¢†åŸŸ/ä»»åŠ¡é€‚åº”ä¸¤ç§ï¼Œå…·ä½“çš„ï¼š

**dev_for_chatmini**ï¼šé€šç”¨æœ€å°å®ç°åˆ†æ”¯ï¼Œå°½å¯èƒ½æ”¯æŒä¸åŒçš„åŸºç¡€æ¨¡å‹å’Œè®­ç»ƒæ–¹å¼ï¼Œæä¾›å¯æ¯”è¾ƒçš„å®ç°ã€‚

1. ã€P0ã€‘ChatGPTæœ€å°å¤ç°ï¼šå®Œæ•´çš„RLHFå¤ç°SFT-RM-PPOï¼Œdoing
2. ã€P0ã€‘é€‚é…ä¸åŒçš„åŸºåº§æ¨¡å‹
3. é€‚é…ä¸åŒçš„PEFTç®—æ³•
4. ã€P2ã€‘æ¢ç´¢æ–°çš„è®­ç»ƒæ–¹å¼
5. ã€P3ã€‘æ¢ç´¢çŸ¥è¯†è¿ç§»ï¼šæ¯”å¦‚è’¸é¦



**dev_for_chatzhihu**ï¼šçŸ¥ä¹åŠé—®ç­”é¢†åŸŸé€‚é…ï¼Œä¸»è¦æƒ³è§£å†³ä¸€äº›çŸ¥ä¹ä½¿ç”¨è¿‡ç¨‹ä¸­çš„ç—›ç‚¹ï¼Œæ¯”å¦‚é—®é¢˜å†—ä½™ã€å›ç­”å¤ªå¤šç­‰ç­‰ã€‚

1. ã€P0ã€‘æ”¶é›†çŸ¥ä¹æ•°æ®æ”¶é›†åŠå¤„ç†
   1. SFTæ•°æ®
   2. RLHFæ•°æ®ï¼šç­”æ¡ˆæ‰“åˆ†
   3. æ‘˜è¦æ•°æ®ï¼šç­”æ¡ˆ/è§‚ç‚¹æ±‡æ€»ã€æ‘˜è¦
2. ã€P0ã€‘åŸºäºçŸ¥ä¹æ•°æ®åšSFT
3. ã€P1ã€‘åŸºäºçŸ¥ä¹æ•°æ®åšRLHF
4. ã€P2ã€‘è¾“å‡ºçŸ¥ä¹LoRA
5. ã€P3ã€‘å’ŒçŸ¥ä¹çƒ­æ¦œèŠå¤©çš„demo



**dev_for_chatzhangsan**ï¼šæ³•å¾‹é¢†åŸŸé€‚é…ï¼Œå¼ ä¸‰çŠ¯äº†ä»€ä¹ˆç½ªï¼Ÿ

1. ã€P0ã€‘æ³•å¾‹é¢†åŸŸæ•°æ®æ”¶é›†åŠå¤„ç†
2. æ³•å¾‹æ¡æ–‡è§£é‡Š
3. ã€P1ã€‘ç½ªååˆ¤å®šï¼šå¼ ä¸‰çŠ¯äº†ä»€ä¹ˆç½ªï¼Ÿ



æ›´å¤šé¢†åŸŸï¼Œæ•¬è¯·æœŸå¾…ï¼

ChatPiXiu-Eat every ChatGPT - Output your own chatbot!

### 2.åŠ å…¥æˆ‘ä»¬

OpenNLPè®¡åˆ’çš„å…¶ä»–å†…å®¹å°šåœ¨ç­¹å¤‡ä¸­ï¼Œæš‚æ—¶åªå¼€æºäº†æœ¬é¡¹ç›®å’Œ[OpenTextClassification](https://github.com/catqaq/OpenTextClassification)é¡¹ç›®ã€‚æ¬¢è¿å¤§å®¶ç§¯æå‚ä¸ChatPiXiuçš„å»ºè®¾å’Œè®¨è®ºï¼Œä¸€èµ·å˜å¾—æ›´å¼ºï¼

åŠ å…¥æ–¹å¼ï¼š

- **é¡¹ç›®å»ºè®¾**ï¼šå¯ä»¥åœ¨å‰é¢åˆ—å‡ºçš„å¼€å‘è®¡åˆ’ä¸­é€‰æ‹©è‡ªå·±æ„Ÿå…´è¶£çš„éƒ¨åˆ†è¿›è¡Œå¼€å‘ï¼Œå»ºè®®ä¼˜å…ˆé€‰æ‹©é«˜ä¼˜å…ˆçº§çš„ä»»åŠ¡ã€‚åŒ…æ‹¬èµ„æ–™è°ƒç ”å’Œç®—æ³•å¼€å‘ç­‰å·¥ä½œã€‚
- OpenLLMæŠ€æœ¯äº¤æµç¾¤ï¼šçŸ¥è¯†åœ¨è®¨è®ºä¸­å‘å±•ï¼ŒQQç¾¤ï¼š740679327
- æŠ€æœ¯åˆ†äº«å’Œè®¨è®ºï¼šè¾“å‡ºå€’é€¼è¾“å…¥ï¼Œæ¬¢è¿æŠ•ç¨¿ï¼Œç¨¿ä»¶ä¼šåŒæ­¥åˆ°æœ¬é¡¹ç›®çš„docsç›®å½•å’ŒçŸ¥ä¹ä¸“æ OpenNLP. åŒæ—¶ä¹Ÿæ¬¢è¿å¤§å®¶ç§¯æçš„å‚ä¸æœ¬é¡¹ç›®çš„è®¨è®ºhttps://github.com/catqaq/ChatPiXiu/discussionsã€‚

## å¼€æºChatGPTè°ƒç ”

### 1.å¼€æºChatGTPå¹³æ›¿

æ³¨ï¼šå¼€æºç±»ChatGPT/LLMæ±‡æ€»ï¼ŒæŒç»­æ›´æ–°ä¸­ï¼Œæ¬¢è¿è´¡çŒ®! ç°å·²è¶…è¿‡60+ï¼

| é¡¹ç›®                                                         | åŸºç¡€æ¨¡å‹                                      | lang        | æœºæ„                                                         | æ•°æ®é›†                                                       | license                                                      | ä»‹ç»                                                         | å¤‡æ³¨                                                         |
| ------------------------------------------------------------ | --------------------------------------------- | ----------- | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
| [LLaMA](https://github.com/facebookresearch/llama)           | LLaMA                                         | Multi       | meta                                                         | CCNet [67%], C4 [15%], GitHub [4.5%], Wikipedia [4.5%], Books [4.5%], ArXiv [2.5%], Stack Exchange[2%]. | [Apache-2.0 license](https://github.com/tatsu-lab/stanford_alpaca/blob/main/LICENSE) | å¯èƒ½æ˜¯ç›®å‰å¼€æºChatGPTç”¨çš„æœ€å¤šçš„åŸºç¡€æ¨¡å‹                      | æ”¯æŒå¤šè¯­è¨€ï¼Œä½†ä»¥è‹±æ–‡ä¸ºä¸»                                     |
| [stanford_alpaca](https://github.com/tatsu-lab/stanford_alpaca) Alpaca | LLaMA                                         | eng         | stanford                                                     | [alpaca_data](https://github.com/tatsu-lab/stanford_alpaca/blob/main/alpaca_data.json) | [Apache-2.0 license](https://github.com/tatsu-lab/stanford_alpaca/blob/main/LICENSE) | æŒ‡ä»¤è°ƒä¼˜çš„ LLaMA æ¨¡å‹: An Instruction-following LLaMA Model. è®© OpenAI çš„ text-davinci-003 æ¨¡å‹ä»¥ self-instruct æ–¹å¼ç”Ÿæˆ 52K æŒ‡ä»¤æ ·æœ¬ï¼ŒSFT | FTæ¨¡å‹è¯­è¨€ä»¥æ•°æ®ä¸ºå‡†                                         |
| [ChatLLaMA](https://github.com/nebuly-ai/nebullvm/tree/main/apps/accelerate/chatllama) | LLaMA                                         |             | Nebuly+AI                                                    | -                                                            | [license](https://github.com/nebuly-ai/nebullvm/blob/main/apps/accelerate/chatllama/LICENSE) | æ•°æ®é›†åˆ›å»ºã€ä½¿ç”¨ RLHF è¿›è¡Œé«˜æ•ˆè®­ç»ƒä»¥åŠæ¨ç†ä¼˜åŒ–ã€‚             |                                                              |
| [Chinese-LLaMA-Alpaca](https://github.com/ymcui/Chinese-LLaMA-Alpaca) | LLaMA                                         | mutli       | [ymcui](https://github.com/ymcui)                            | -                                                            | [Apache-2.0 license](https://github.com/ymcui/Chinese-LLaMA-Alpaca/blob/main/LICENSE.md) | Chinese LLaMA & Alpaca LLMs; ä¸­æ–‡è¯è¡¨æ‰©å……                    |                                                              |
| [alpaca-lora](https://github.com/tloen/alpaca-lora)          | LLaMA                                         |             | stanford                                                     | [LLaMA-GPT4 dataset](https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM) | [Apache-2.0 license](https://github.com/tloen/alpaca-lora/blob/main/LICENSE) | [LoRA](https://zhuanlan.zhihu.com/p/620327907)               |                                                              |
| Chinese-alpaca-lora Luotuo-Chinese-LLM                       | LLaMA                                         |             | -                                                            |                                                              |                                                              | LoRA                                                         |                                                              |
| [ChatGLM](https://github.com/THUDM/ChatGLM-6B)               | GLM                                           | cn/eng      | æ¸…å                                                         | 1T æ ‡è¯†ç¬¦çš„ä¸­è‹±åŒè¯­æ•°æ®                                      | [Apache-2.0 license](https://github.com/THUDM/ChatGLM-6B/blob/main/LICENSE) | ç›‘ç£å¾®è°ƒã€åé¦ˆè‡ªåŠ©ã€äººç±»åé¦ˆå¼ºåŒ–å­¦ä¹                          | [PROJECT.md](https://github.com/THUDM/ChatGLM-6B/blob/main/PROJECT.md) |
| [FastChat](https://github.com/lm-sys/FastChat)  Vicuna       | LLaMA                                         | eng         | UC Berkeley, CMU, Stanford, UCSD and MBZUAI                  | ShareGPT, 70ké—®ç­”æŒ‡ä»¤æ•°æ®                                    | [Apache-2.0 license](https://github.com/lm-sys/FastChat/blob/main/LICENSE) | SFTï¼Œä½¿ç”¨GPT-4ä½œä¸ºè¯„åˆ¤æ ‡å‡†ï¼Œç»“æœæ˜¾ç¤ºVicuna-13Båœ¨è¶…è¿‡90%çš„æƒ…å†µä¸‹å®ç°äº†ä¸ChatGPTå’ŒBardç›¸åŒ¹æ•Œçš„èƒ½åŠ›ã€‚ |                                                              |
| Chinese-Vicuna                                               | LLaMA                                         | cn          | -                                                            | -                                                            | [Apache-2.0 license](https://github.com/Facico/Chinese-Vicuna/blob/master/LICENSE) | LoRA                                                         |                                                              |
| [EasyLM](https://github.com/young-geng/EasyLM) Koalaè€ƒæ‹‰     | LLaMA multi                                   | eng         | UCä¼¯å…‹åˆ©                                                     | ChatGPTæ•°æ®å’Œå¼€æºæ•°æ®ï¼ˆOpen Instruction Generalist (OIG)ã€æ–¯å¦ç¦ Alpaca æ¨¡å‹ä½¿ç”¨çš„æ•°æ®é›†ã€Anthropic HHã€OpenAI WebGPTã€OpenAI Summarizationï¼‰ | [Apache-2.0 license](https://github.com/Facico/Chinese-Vicuna/blob/master/LICENSE) | SFT/13B/500kæ¡æ•°æ®                                           |                                                              |
| [ColossalChat](https://github.com/hpcaitech/ColossalAI/tree/main/applications/Chat) | LLaMA                                         |             | ColossalAI                                                   | [InstructionWild](https://github.com/XueFuzhao/InstructionWild):104K bilingual datasets | [LICENSE](https://github.com/hpcaitech/ColossalAI/blob/main/applications/Chat/LICENSE) | SFT-RM-RLHF                                                  |                                                              |
| [ChatRWKV](https://github.com/BlinkDL/ChatRWKV)              | RWKV                                          |             | [BlinkDL](https://github.com/BlinkDL)                        | -                                                            | [Apache-2.0 license](https://github.com/BlinkDL/ChatRWKV/blob/main/LICENSE) | ChatRWKV is like ChatGPT but powered by RWKV (100% RNN) language model, and open source. |                                                              |
| [ChatYuan](https://github.com/clue-ai/ChatYuan)              | T5                                            | eng/cn      | å…ƒè¯­æ™ºèƒ½                                                     | PromptClue                                                   | [LICENSE](https://github.com/clue-ai/ChatYuan/blob/main/LICENSE) | åŸºäºPromptClueè¿›è¡Œäº†ç›‘ç£å¾®è°ƒ                                 |                                                              |
| [OpenChatKit](https://github.com/togethercomputer/OpenChatKit) | GPT-NoX-20B                                   |             | Together+LAION+Ontocord.ai                                   | OIG-43M                                                      | [Apache-2.0 license](https://github.com/togethercomputer/OpenChatKit/blob/main/LICENSE) | 60äº¿å‚æ•°çš„å®¡æ ¸æ¨¡å‹ï¼Œå¯¹ä¸åˆé€‚æˆ–è€…æ˜¯æœ‰å®³çš„ä¿¡æ¯è¿›è¡Œè¿‡æ»¤         |                                                              |
| BELLE                                                        | Bloom LLama                                   | cn          | [LianjiaTech](https://github.com/LianjiaTech)                | 10M-ChatGPTç”Ÿæˆçš„æ•°æ®                                        | [Apache-2.0 license](https://github.com/LianjiaTech/BELLE/blob/main/LICENSE) | SFT                                                          |                                                              |
| PaLM-rlhf-pytorch                                            | PaLM                                          |             | [lucidrains](https://github.com/lucidrains)                  | -                                                            | [MIT license](https://github.com/lucidrains/PaLM-rlhf-pytorch/blob/main/LICENSE) | RLHF                                                         | PaLMå¤ªå¤§äº†                                                   |
| [dolly](https://github.com/databrickslabs/dolly)             | v1:GPT-J-6B v2:pythia                         | eng         | Databricks                                                   | The Pile+databricks-dolly-15k                                | [MIT license](https://github.com/lucidrains/PaLM-rlhf-pytorch/blob/main/LICENSE) | å‚è€ƒAlpaca; dolly-v2-12b based on pythia-12b                 |                                                              |
| LMFlow                                                       | LLaMA                                         |             | [OptimalScale](https://github.com/OptimalScale)              |                                                              |                                                              | An Extensible Toolkit for Finetuning and Inference of Large Foundation Models. Large Model for All. LLaMA-7Bï¼Œä¸€å¼ 3090è€—æ—¶ 5 ä¸ªå°æ—¶ |                                                              |
| GPTrillion                                                   | -                                             |             | -                                                            | -                                                            | -                                                            | 1.5ä¸‡äº¿ï¼Œå¤šæ¨¡æ€                                              |                                                              |
| [open_flamingo](https://github.com/mlfoundations/open_flamingo) | LLaMA CLIP                                    |             | LAION                                                        | [Multimodal C4](https://github.com/allenai/mmc4)             | [MIT license](https://github.com/mlfoundations/open_flamingo/blob/main/LICENSE) |                                                              |                                                              |
| [baize-chatbot](https://github.com/project-baize/baize-chatbot) | LLaMA                                         | eng         | [project-baize](https://github.com/project-baize)            | 100k dialogs generated by letting ChatGPT chat with itself.  | [GPL-3.0 license](https://github.com/project-baize/baize-chatbot/blob/main/LICENSE) | LoRA                                                         |                                                              |
| [ChatPiXiu](https://github.com/catqaq/ChatPiXiu)             | multi                                         |             | ç¾¡é±¼æ™ºèƒ½                                                     | -                                                            | -                                                            | LoRA                                                         | ç­¹å¤‡é˜¶æ®µ                                                     |
| [stackllama](https://huggingface.co/blog/stackllama)         | LLaMA                                         |             | Hugging Face                                                 | -                                                            | -                                                            | ç”¨RLHFè®­ç»ƒLLaMAçš„å®è·µæŒ‡å—                                    |                                                              |
| [Lit-LLaMA](https://github.com/Lightning-AI/lit-llama)       | LLaMA                                         | multi       | lightening-ai                                                | -                                                            | [Apache-2.0 license](https://github.com/Lightning-AI/lit-llama/blob/main/LICENSE) | é‡å†™é‡è®­LLaMA,ç»•å¼€license: Implementation of the LLaMA language model based on nanoGPT. | å¯å•†ç”¨ç‰ˆLLaMA                                                |
| [OPT](https://arxiv.org/abs/2205.01068)                      | OPT                                           | eng         | meta                                                         | -                                                            | [MIT license](https://github.com/facebookresearch/metaseq/blob/main/LICENSE) | å½“å¹´å¯¹æ ‡GPT3çš„æ¨¡å‹                                           |                                                              |
| [Cerebras-GPT](https://huggingface.co/cerebras/Cerebras-GPT-13B) | Cerebras-GPT                                  | eng         | Cerebras                                                     | The Pile                                                     | [Apache-2.0 license](https://github.com/Cerebras/modelzoo/blob/main/LICENSE) | GPT-3 style; æœ€å° 1.11 äº¿ï¼Œæœ€å¤§ 130 äº¿ï¼Œå…± 7 ä¸ªæ¨¡å‹          |                                                              |
| BLOOM                                                        | BLOOM                                         | multi       | [bigscience](https://huggingface.co/bigscience)              | Total seen tokens: 366B                                      | ä»£ç ï¼š[Apache-2.0 license ](https://github.com/huggingface/transformers-bloom-inference/blob/main/LICENSE)æ¨¡å‹ï¼šRAIL License v1.0 | 176Bï¼›46 ç§è‡ªç„¶è¯­è¨€ï¼ˆåŒ…æ‹¬ä¸­æ–‡ï¼‰å’Œ 13 ç§ç¼–ç¨‹è¯­è¨€              |                                                              |
| GPT-J                                                        | GPT-3                                         | multi       | EleutherAI                                                   | The Pile                                                     | [apache-2.0](https://huggingface.co/models?license=license:apache-2.0) | based on GPT-3;                                              |                                                              |
| GPT-2                                                        |                                               |             |                                                              |                                                              |                                                              |                                                              |                                                              |
| [RWKV](https://github.com/BlinkDL/RWKV-LM)                   | [RWKV-LM](https://github.com/BlinkDL/RWKV-LM) | cn/eng      | [BlinkDL](https://github.com/BlinkDL)                        |                                                              |                                                              |                                                              | çº¯RNN                                                        |
| [é¹ç¨‹ãƒ»ç›˜å¤ Î±](https://www.oschina.net/p/pangu-alpha)        |                                               | cn          | é¹åŸ                                                         | 2TB                                                          | [Apache License 2.0](https://openi.pcl.ac.cn/PCL-Platform.Intelligence/PanGu-Alpha/src/branch/master/LICENSE) |                                                              |                                                              |
| [é¹ç¨‹ãƒ»ç›˜å¤å¯¹è¯](https://www.oschina.net/p/pangu-dialog)     |                                               | cn          | é¹åŸ                                                         |                                                              |                                                              |                                                              |                                                              |
| [æ‚Ÿé“](https://www.oschina.net/p/wudao-model)                |                                               | cn/eng      | BAAI(æ™ºæº)                                                   |                                                              |                                                              | å¤šæ¨¡æ€; 1.75 ä¸‡äº¿å‚æ•°; å›¾æ–‡ï¼šCogViewã€BriVLï¼›æ–‡æœ¬ï¼šGLMã€CPMã€Transformer-XLã€EVAã€Lawformerï¼›ç”Ÿç‰©ï¼šProtTrans |                                                              |
| [MOSS](https://www.oschina.net/p/moss)                       | MOSS                                          | cn/eng      | [OpenLMLab](https://github.com/OpenLMLab)                    | 700B tokens                                                  | ä»£ç Apache 2.0ï¼Œæ•°æ®CC BY-NC 4.0ï¼Œæ¨¡å‹æƒé‡GNU AGPL 3.0       | æ”¯æŒä¸­è‹±åŒè¯­å’Œå¤šç§æ’ä»¶; åŸºåº§moss-moon-003-base               |                                                              |
| [ä¼¶è” (Linly)](https://www.oschina.net/p/linly)              | LLaMA                                         | cn          | [CVI-SZU](https://github.com/CVI-SZU)                        |                                                              | Apache Licence 2.0                                           | 33B çš„ Linly-Chinese-LLAMA æ˜¯ç›®å‰æœ€å¤§çš„ä¸­æ–‡ LLaMA æ¨¡å‹       |                                                              |
| [åé©¼ (HuaTuo)](https://www.oschina.net/p/huatuo-llama)      | LLaMA                                         | cn/eng      | [SCIR-HI](https://github.com/SCIR-HI)                        |                                                              | [Apache-2.0 license](https://github.com/SCIR-HI/Huatuo-Llama-Med-Chinese/blob/main/LICENSE) |                                                              | åŒ»å­¦                                                         |
| [BBT-2](https://www.oschina.net/p/bbt-2)                     |                                               | cn/eng      |                                                              |                                                              |                                                              | 120 äº¿å‚æ•°çš„é€šç”¨å¤§è¯­è¨€æ¨¡å‹                                   |                                                              |
| [CodeGeeX](https://www.oschina.net/p/codegeex)               | -                                             | code        | é¹åŸ                                                         |                                                              |                                                              | 130 äº¿å‚æ•°çš„å¤šç¼–ç¨‹è¯­è¨€ä»£ç ç”Ÿæˆé¢„è®­ç»ƒæ¨¡å‹                     | code                                                         |
| [RedPajama](https://www.oschina.net/p/redpajama)             | gpt-neox                                      | eng         | Togetherã€Ontocord.aiã€ETH DS3Labã€æ–¯å¦ç¦å¤§å­¦ CRFMã€Hazy Research å’Œ MILA é­åŒ—å…‹ AI ç ”ç©¶æ‰€ | 800B/1T                                                      | Apache-2.0                                                   | å¼€æºåœ°å…¨é¢å¯¹é½LLaMAçš„è®­ç»ƒæ•°æ®é›†                              |                                                              |
| [OpenAssistant](https://www.oschina.net/p/open-assistant)    |                                               | eng         | [LAION-AI](https://github.com/LAION-AI)                      |                                                              | [Apache-2.0 license](https://github.com/LAION-AI/Open-Assistant/blob/main/LICENSE) | OpenAssistant is a chat-based assistant that understands tasks, can interact with third-party systems, and retrieve information dynamically to do so. |                                                              |
| [StableLM](https://www.oschina.net/p/stablelm)               | pythia                                        | eng         | [Stability-AI](https://github.com/Stability-AI)              |                                                              | [Apache-2.0 license](https://github.com/Stability-AI/StableLM/blob/main/LICENSE) | Stability AI Language Models; max len 4096                   |                                                              |
| [StarCoder](https://www.oschina.net/p/starcoder)             |                                               | code        | [bigcode-project](https://github.com/bigcode-project)        |                                                              | [Apache-2.0 license](https://github.com/bigcode-project/starcoder/blob/main/LICENSE) |                                                              | code                                                         |
| [SantaCoder](https://www.oschina.net/p/santacoder)           |                                               | code        | [bigcode](https://huggingface.co/bigcode)                    | The Stackï¼ˆv1.1ï¼‰                                            | the BigCode OpenRAIL-M v1 license                            | è½»é‡çº§ AI ç¼–ç¨‹æ¨¡å‹ï¼Œ1.1B                                     | code                                                         |
| [MLC LLM](https://www.oschina.net/p/mlc-llm)                 |                                               | -           | [mlc-ai](https://github.com/mlc-ai)                          | -                                                            | [Apache-2.0 license](https://github.com/mlc-ai/mlc-llm/blob/main/LICENSE) | æœ¬åœ°å¤§è¯­è¨€æ¨¡å‹éƒ¨ç½²ï¼›Enable everyone to develop, optimize and deploy AI models natively on everyone's devices. |                                                              |
| [Web LLM](https://www.oschina.net/p/web-llm)                 |                                               |             | [mlc-ai](https://github.com/mlc-ai)                          |                                                              | [Apache-2.0 license](https://github.com/mlc-ai/web-llm/blob/main/LICENSE) | Bringing large-language models and chat to web browsers.     |                                                              |
| [WizardLM](https://www.oschina.net/p/wizardlm)               | LLaMA                                         | eng         | [nlpxucan](https://github.com/nlpxucan)                      |                                                              |                                                              | Evol-Instruct                                                |                                                              |
| [YaLM 100B](https://www.oschina.net/p/yalm-100b)             |                                               | eng/russian | [yandex](https://github.com/yandex)                          |                                                              | [Apache-2.0 license](https://github.com/yandex/YaLM-100B/blob/main/LICENSE) |                                                              |                                                              |
| [OpenLLaMA](https://www.oschina.net/p/openllama)             |                                               | multi       | [s-JoL](https://github.com/s-JoL)                            |                                                              | [MIT license](https://github.com/s-JoL/Open-Llama/blob/main/LICENSE) | LLaMA å¼€æºå¤ç°ç‰ˆ                                             |                                                              |
| BiLLa                                                        | LLaMA                                         | cn/eng      | [Neutralzz](https://github.com/Neutralzz)                    |                                                              |                                                              | A Bilingual LLaMA with Enhanced Reasoning Ability            |                                                              |
| pandallm                                                     | LLaMA                                         | cn/eng      | [dandelionsllm](https://github.com/dandelionsllm)            |                                                              | Apache-2.0 license                                           |                                                              |                                                              |
| pandalm                                                      |                                               |             | [WeOpenML](https://github.com/WeOpenML)                      |                                                              | [Apache-2.0 license](https://github.com/WeOpenML/PandaLM/blob/main/LICENSE) | PandaLMï¼šå¯é‡ç°å’Œè‡ªåŠ¨åŒ–çš„è¯­è¨€æ¨¡å‹è¯„ä¼°                        |                                                              |
| [gpt4all](https://github.com/nomic-ai/gpt4all)               |                                               |             | [nomic-ai](https://github.com/nomic-ai)                      |                                                              | [MIT license](https://github.com/nomic-ai/gpt4all/blob/main/LICENSE.txt) | gpt4all: an ecosystem of open-source chatbots trained on a massive collections of clean assistant data including code, stories and dialogue |                                                              |
| [stable-vicuna](https://huggingface.co/CarperAI/stable-vicuna-13b-delta) |                                               | eng         | [CarperAI](https://huggingface.co/CarperAI)                  |                                                              | [CC-BY-NC-SA-4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/) | StableVicuna-13B is a Vicuna-13B v0 model fine-tuned         |                                                              |
| [MPT](https://huggingface.co/mosaicml/mpt-7b)                | MPT                                           | eng         | [mosaicml](https://huggingface.co/mosaicml)                  | 1T tokens of English text and code                           | Apache-2.0                                                   | ALiBiä¿è¯äº†è‰¯å¥½çš„é•¿åº¦å¤–æ¨æ€§                                  |                                                              |
| ImageBind                                                    |                                               | å¤šæ¨¡æ€      | meta                                                         |                                                              |                                                              | One embedding space to bind them all.                        |                                                              |
| [Phoenix](https://github.com/FreedomIntelligence/LLMZoo)     |                                               | multi       | CUHK                                                         |                                                              |                                                              | 7B/BLOOMZ + å¾®è°ƒ                                             |                                                              |
| [ChatPLUG](https://github.com/X-PLUG/ChatPLUG)               |                                               |             | Alibaba                                                      |                                                              |                                                              | Encoder-Decoder/3.7B                                         |                                                              |
| [BLOOMZ](https://github.com/bigscience-workshop/xmtf)        |                                               | multi       | BigScience                                                   |                                                              |                                                              | BLOOM + å¤šä»»åŠ¡å¾®è°ƒ                                           |                                                              |
| [CPM-Ant+](https://github.com/OpenBMB/CPM-Live/tree/cpm-ant-plus/cpm-live) |                                               | cn/eng      | OpenBMB                                                      |                                                              |                                                              | 10B/Decoder-only(UniLM)                                      |                                                              |
| [PaLM](https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html) |                                               | multi       | Google                                                       |                                                              | -                                                            | pathways/540B                                                | æœªå¼€æº                                                       |
| [PaLM 2](https://blog.google/technology/ai/google-palm-2-ai-large-language-model/) |                                               | multi       | Google                                                       |                                                              | -                                                            | pathways; æ”¹è¿›çš„å¤šè¯­è¨€ã€æ¨ç†å’Œç¼–ç èƒ½åŠ›                       | æœªå¼€æº                                                       |
|                                                              |                                               |             |                                                              |                                                              |                                                              |                                                              |                                                              |



### 2.åŸºç¡€æ¨¡å‹

æ³¨ï¼šåŸºç¡€LLMæ±‡æ€»ï¼ŒæŒç»­æ›´æ–°ä¸­ï¼Œæ¬¢è¿è´¡çŒ®! ç°å·²è¶…è¿‡15+ï¼ä¸ªäººçš„å·¥ä½œå’Œç ”ç©¶å…´è¶£ä¼šæ›´å…³æ³¨åŸºç¡€æ¨¡å‹ç›¸å…³æŠ€æœ¯åŠå…¶åº”ç”¨ï¼

| model                                                        | Architecture/task           | lang   | tokenizer | vocab                                    | PE                   | max len                        | size                                    | org                                           | data                                                         | license                                                      | intro                                                        | notes                                                        |
| ------------------------------------------------------------ | --------------------------- | ------ | --------- | ---------------------------------------- | -------------------- | ------------------------------ | --------------------------------------- | --------------------------------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
| [LLaMA](https://huggingface.co/decapoda-research/llama-7b-hf) | decoder/LM                  | multi  | BBPE      | 32K                                      | RoPE                 | 2048                           | 7B/13B/33B/65B                          | meta                                          | 1.4ä¸‡äº¿ tokens                                               | [GPL-3.0 license](https://github.com/facebookresearch/llama/blob/main/LICENSE) | Meta å¤§è¯­è¨€æ¨¡å‹: The model comes in different sizes: 7B, 13B, 33B and 65B parameters. |                                                              |
| GLM                                                          | mix/è‡ªå›å½’å¼å¡«ç©ºï¼Œprefix LM | cn/eng | multi     | multi                                    | RoPE                 | 2048 for 130B 1024 for ChatGLM | 6B/10B/130B                             | æ™ºè°±                                          | ä¸­è‹±æ–‡tokenå„2000äº¿                                          | [Apache-2.0 license](https://github.com/THUDM/ChatGLM-6B/blob/main/LICENSE) | ChatGLM-6B: 1T data                                          |                                                              |
| GPT1                                                         | decoder/LM                  | eng    | BPE       | 40478                                    | learned              | 512                            |                                         | OpenAI                                        | BooksCorpus/5 G                                              | [MIT License](https://github.com/openai/finetune-transformer-lm/blob/master/LICENSE) | GPTç³»åˆ—çš„èµ·æº                                                | åˆ†è¯éƒ¨åˆ†æœ‰ç‰¹æ®Šå¤„ç†ï¼Œè¯¦è§tokenization_openai.pyï¼š - lowercases all inputs,     - uses `SpaCy` tokenizer and `ftfy` for pre-BPE tokenization if they are installed, fallback to BERT's       `BasicTokenizer` if not. |
| [GPT2](https://huggingface.co/gpt2-xl)                       | decoder/LM                  | multi  | BBPE      | 50257                                    | learned              | 1024                           | 124M/355M/774M/1.5B                     | OpenAI                                        | WebText/ 40G                                                 | [Modified MIT License](https://github.com/openai/gpt-2/blob/master/LICENSE) | ä¸»æ‰“zero-shot                                                | GPTç³»åˆ—å¼€æºçš„æœ€åä¸€èˆ                                        |
| GPT3                                                         | decoder/LM                  | multi  | BBPE      | 50257/davinci                            | learned              | 2048                           | 175B                                    | OpenAI                                        | 570G                                                         |                                                              | sparse attentionï¼›ä¸»æ‰“few-shot/in-context learning           | å…³ä¸Šäº†GPTç³»åˆ—çš„å¤§é—¨                                          |
| GPT3.5                                                       | decoder/LM                  | multi  | BBPE      | 100256                                   | ~learned             | 4096                           |                                         | OpenAI                                        | -                                                            | -                                                            | InstructGPTç­‰ä¸€ç³»åˆ—æ¨¡å‹                                      | æœªå¼€æº                                                       |
| GPT4                                                         | decoder/LM                  | multi  | BBPE      | 100256                                   | ~learned             | 32768                          |                                         | OpenAI                                        | -                                                            | -                                                            | å¤šæ¨¡æ€                                                       | æœªå¼€æº                                                       |
| [BLOOM](https://huggingface.co/bigscience/bloom)             | decoder/LM                  | multi  | BBPE      | 250,680                                  | ALiBi                | 2048                           | 176B                                    | bigscience                                    | [Data Cards](https://huggingface.co/spaces/bigscience/BigScienceCorpus) | RAIL License v1.0                                            | Modified from Megatron-LM GPT2; StableEmbedding              | åŒ…æ‹¬Training logs                                            |
| PaLM                                                         |                             |        |           |                                          | RoPE                 |                                |                                         |                                               |                                                              |                                                              |                                                              |                                                              |
| Chinchilla                                                   |                             |        |           |                                          | transformer-XL style |                                |                                         |                                               |                                                              |                                                              |                                                              |                                                              |
| OPT                                                          |                             | eng    |           |                                          | learned              |                                |                                         |                                               |                                                              |                                                              |                                                              |                                                              |
| [GPT-J](https://huggingface.co/EleutherAI/gpt-j-6b)          | decoder/LM                  | multi  | BBPE      | 50257/50400â€  (same tokenizer as GPT-2/3) | RoPE                 | 2048                           | 6B                                      | EleutherAI                                    | The Pile                                                     | [apache-2.0](https://huggingface.co/models?license=license:apache-2.0) | based on GPT-3;                                              |                                                              |
| [Lit-LLaMA](https://github.com/Lightning-AI/lit-llama)       |                             |        |           |                                          |                      |                                |                                         |                                               |                                                              |                                                              | Implementation of the LLaMA language model based on nanoGPT. |                                                              |
| [Cerebras-GPT](https://huggingface.co/cerebras/Cerebras-GPT-13B) | decoder                     | eng    | BPE       | 50257                                    | Learned              | 2048                           |                                         | [Cerebras Systems](https://www.cerebras.net/) | The Pile                                                     | Apache 2.0                                                   | The family includes 111M, 256M, 590M, 1.3B, 2.7B, 6.7B, and 13B models. All models in the Cerebras-GPT family have been trained in accordance with Chinchilla scaling laws (20 tokens per model parameter) which is compute-optimal. |                                                              |
| [RWKV](https://github.com/BlinkDL/RWKV-LM)                   | RNN                         | eng/cn |           |                                          | None(pure RNN)       | 1024/4096/8192                 | [multi](https://huggingface.co/BlinkDL) |                                               | the Pile                                                     |                                                              | ç»“åˆäº† RNN å’Œ Transformer çš„è¯­è¨€æ¨¡å‹ï¼Œé€‚åˆé•¿æ–‡æœ¬ï¼Œè¿è¡Œé€Ÿåº¦è¾ƒå¿«ï¼Œæ‹Ÿåˆæ€§èƒ½è¾ƒå¥½ï¼Œå ç”¨æ˜¾å­˜è¾ƒå°‘ï¼Œè®­ç»ƒç”¨æ—¶è¾ƒå°‘ã€‚RWKV æ•´ä½“ç»“æ„ä¾ç„¶é‡‡ç”¨ Transformer Block çš„æ€è·¯ï¼Œç›¸è¾ƒäºåŸå§‹ Transformer Block çš„ç»“æ„ï¼ŒRWKV å°† self-attention æ›¿æ¢ä¸º Position Encoding å’Œ TimeMixï¼Œå°† FFN æ›¿æ¢ä¸º ChannelMixã€‚å…¶ä½™éƒ¨åˆ†ä¸ Transfomer ä¸€è‡´ã€‚ |                                                              |
| CoLT5                                                        |                             |        |           |                                          | T5 bias style        |                                |                                         |                                               |                                                              |                                                              |                                                              |                                                              |
| MOSS                                                         | decoder/LM                  | cn/eng |           |                                          |                      | 2048                           | 16B                                     | [OpenLMLab](https://github.com/OpenLMLab)     | 700B tokens                                                  | ä»£ç Apache 2.0ï¼Œæ•°æ®CC BY-NC 4.0ï¼Œæ¨¡å‹æƒé‡GNU AGPL 3.0       | æ”¯æŒä¸­è‹±åŒè¯­å’Œå¤šç§æ’ä»¶                                       |                                                              |
| [MPT](https://huggingface.co/mosaicml/mpt-7b)                | decoder/LM                  | eng    | BBPE      | 50432                                    | ALiBi                | 2048/65k/84k                   | 7B                                      | [mosaicml](https://huggingface.co/mosaicml)   | 1T tokens+å„ç§FTæ•°æ®                                         | Apache-2.0                                                   | Although the model was trained with a sequence length of 2048, ALiBi enables users to increase the maximum sequence length during finetuning and/or inference.  The model vocabulary size of 50432 was set to be a multiple of 128 (as in MEGATRON-LM) |                                                              |
| [mt5](https://huggingface.co/google/mt5-xxl)                 | encoder-decoder             | multi  |           |                                          |                      |                                | 1.2B/3.7B/13B                           | Google                                        | mC4                                                          |                                                              |                                                              |                                                              |
| [Wenzhong2.0-GPT2-3.5B-chinese](https://huggingface.co/IDEA-CCNL/Wenzhong2.0-GPT2-3.5B-chinese) | decoder/LM                  | cn     |           |                                          |                      |                                | 3.5B                                    | IDEA-CCNL                                     |                                                              |                                                              |                                                              |                                                              |
| CPM-Generate                                                 |                             | cn     |           |                                          |                      |                                | 2.6B                                    | TsinghuaAI                                    | 100GB Chinese training data                                  |                                                              |                                                              |                                                              |
| [bloom-zh](https://huggingface.co/Langboat/bloom-1b4-zh)     | decoder/LM                  | cn     | BBPE      | 46145                                    | ALiBi                | 2048                           | 1.4B/2.5B/6.4B                          | Langboat                                      | -                                                            |                                                              | è¯è¡¨è£å‰ªï¼Œä¿ç•™ä¸­æ–‡: 250880 to 46145                          |                                                              |
| [GPT-2B](https://huggingface.co/nvidia/GPT-2B-001)           | decoder/LM                  | eng    | BBPE      |                                          | RoPE                 | 4096                           |                                         | HuggingFace+Nvidia                            | 1.1T tokens                                                  |                                                              |                                                              |                                                              |
|                                                              |                             |        |           |                                          |                      |                                |                                         |                                               |                                                              |                                                              |                                                              |                                                              |



### 3.æ•°æ®

| dataset                     | type        | æœºæ„                                | å¤§å° | license | ä»‹ç» | å¤‡æ³¨ |
| --------------------------- | ----------- | ----------------------------------- | ---- | ------- | ---- | ---- |
| alpaca_data                 | Instruction | stanford                            |      |         |      |      |
| alpaca_chinese_dataset      | \-          | hikariming                          |      |         |      |      |
| Multilingual Instruction    |             | Guanaco                             |      |         |      |      |
| alpaca_chinese_dataset      |             | carbonz0                            |      |         |      |      |
| 0.5M+1M chinese instruction |             | LianjiaTech                         |      |         |      |      |
| shareGPT                    |             | [lm-sys](https://github.com/lm-sys) |      |         |      |      |
|                             |             |                                     |      |         |      |      |



### 4.äº§å“

æ³¨ï¼šä¸ºäº†æ–‡æ¡£çš„å®Œæ•´æ€§ï¼Œå°†å·¥ä¸šç•Œçš„ChatGPTä¹Ÿè¿›è¡Œäº†æ±‡æ€»ï¼Œåªåšä»‹ç»ä¸åšæ¯”è¾ƒï¼Œä»¥å…äº‰è®®!

| model                                             | org       | intro | notes |
| ------------------------------------------------- | --------- | ----- | ----- |
| [ChatGPT-GPT-3.5-turbo](https://chat.openai.com/) | OpenAI    |       |       |
| [ChatGPT-GPT-4](https://chat.openai.com/)         | OpenAI    |       |       |
| Claude                                            | Anthropic |       |       |
| æ–‡å¿ƒä¸€è¨€                                          | ç™¾åº¦      |       |       |
| æ˜Ÿç«äººçŸ¥å¤§æ¨¡å‹                                    | è®¯é£      |       |       |
| ChatGLM                                           | æ¸…å/æ™ºè°± |       |       |
| MiniMax                                           | MiniMax   |       |       |
| é€šä¹‰åƒé—®                                          | é˜¿é‡Œ      |       |       |
| Bard                                              | Google    |       |       |
|                                                   |           |       |       |



### 5.è®­ç»ƒ&éƒ¨ç½²

#### 5.1 è®­ç»ƒ

| æ¡†æ¶                                                         | type    | æœºæ„                                      | å…¼å®¹æ€§                       | license                                                      | ä»‹ç»                                                         | å¤‡æ³¨ |
| ------------------------------------------------------------ | ------- | ----------------------------------------- | ---------------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ | ---- |
| [ColossalAI](https://github.com/hpcaitech/ColossalAI)        | general | [hpcaitech](https://github.com/hpcaitech) | é«˜                           | [Apache-2.0 license](https://github.com/hpcaitech/ColossalAI/blob/main/LICENSE) | Colossal-AI: Making large AI models cheaper, faster, and more accessible<br>æ”¯æŒChatGPTå®Œæ•´å¤ç° |      |
| [RLHF](https://github.com/sunzeyeah/RLHF)                    | RL      | [sunzeyeah](https://github.com/sunzeyeah) | åŸºäºtransformersåº“å®ç°       | \-                                                           | Implementation of Chinese ChatGPT.<br>SFTã€Reward Modelå’ŒRLHF |      |
| trlx                                                         | RL      | [CarperAI](https://github.com/CarperAI)   | å¼ºå¤§çš„transformer å¼ºåŒ–å­¦ä¹ åº“ | [MIT license](https://github.com/CarperAI/trlx/blob/main/LICENSE) | A repo for distributed training of language models with Reinforcement Learning via Human Feedback (RLHF)<br>ä¸æ”¯æŒè‡ªå®šä¹‰é¢„è®­ç»ƒæ¨¡å‹ã€‚ |      |
| [trl](https://github.com/lvwerra/trl)                        | RL      | Hugging Face                              | åŸºäºtransformers             | [Apache-2.0 license](https://github.com/lvwerra/trl/blob/main/LICENSE) | åªè¦æ˜¯åŸºäºransformers åº“å¼€å‘çš„é¢„è®­ç»ƒåº“ï¼Œå‡å¯é€‚é…ï¼Œå¼ºçƒˆæ¨è   |      |
| [DeepSpeed-Chat](https://github.com/microsoft/DeepSpeedExamples/tree/master/applications/DeepSpeed-Chat) | general | [microsoft](https://github.com/microsoft) | åŸºäºDeepSpeed                | [Apache-2.0 license](https://github.com/microsoft/DeepSpeedExamples/blob/master/LICENSE) | **è®­ç»ƒé€Ÿåº¦**å¤§å¹…æå‡                                         |      |
| [nanoGPT](https://github.com/karpathy/nanoGPT)               | GPT     | [karpathy](https://github.com/karpathy)   |                              | [MIT license](https://github.com/karpathy/nanoGPT/blob/master/LICENSE) | The simplest, fastest repository for training/finetuning medium-sized GPTs. |      |
|                                                              |         |                                           |                              |                                                              |                                                              |      |



#### 5.2 éƒ¨ç½²



## ä½¿ç”¨æ­¥éª¤ï¼šTODO

1.å…‹éš†æœ¬é¡¹ç›®

`git clone https://github.com/catqaq/ChatPiXiu.git`

2.å‡†å¤‡æ•°æ®



3.è¿è¡Œç¤ºä¾‹



## ç»“æœå±•ç¤º




## å¸¸è§æŠ¥é”™



## å‚è€ƒèµ„æ–™&è‡´è°¢

ã€OpenLLM 011ã€‘ChatPiXiué¡¹ç›®-å¯èƒ½æ˜¯å…¨ç½‘æœ€å…¨çš„ChatGPTå¤ç°è°ƒç ”ï¼š54+å¼€æºChatGPTå¹³æ›¿é¡¹ç›®ï¼Œ15+åŸºç¡€æ¨¡å‹ï¼Œ8+ ChatGPTäº§å“ï¼ - ç¾¡é±¼æ™ºèƒ½çš„æ–‡ç«  - çŸ¥ä¹ https://zhuanlan.zhihu.com/p/629364056 

å¼€æºChatGPTæ›¿ä»£æ¨¡å‹é¡¹ç›®æ•´ç†https://zhuanlan.zhihu.com/p/618790279

å¹³æ›¿chatGPTçš„å¼€æºæ–¹æ¡ˆ https://zhuanlan.zhihu.com/p/618926239ChatGPT/GPT4

å¼€æºâ€œå¹³æ›¿â€æ±‡æ€»https://zhuanlan.zhihu.com/p/621324917

å®Œæ•´ç‰ˆ ChatGPT å…‹éš†æ–¹æ¡ˆï¼Œå¼€æºäº†ï¼https://zhuanlan.zhihu.com/p/617996976

ColossalChatï¼šå®Œæ•´RLHFå¹³æ›¿ChatGPTçš„å¼€æºæ–¹æ¡ˆ https://zhuanlan.zhihu.com/p/618048558ChatGPT

å¼€æºå¹³æ›¿æ¥äº†ï¼Œå¼€ç®±å³ç”¨ï¼å‰OpenAIå›¢é˜Ÿæ‰“é€ ï¼ŒGitHubåˆšå‘å¸ƒå°±æ½è·800+æ˜Ÿhttps://zhuanlan.zhihu.com/p/613556853

LoRA:å¤§æ¨¡å‹çš„ä½ç§©é€‚é…-æœ€è¿‘å¤§ç«çš„loraåˆ°åº•æ˜¯ä»€ä¹ˆä¸œè¥¿ï¼Ÿä¸ºå•¥stable diffusionå’Œå¼€æºChatGPTå¤ç°éƒ½åœ¨ç”¨ï¼Ÿhttps://zhuanlan.zhihu.com/p/620327907? 

æˆæœ¬ä¸åˆ°100ç¾å…ƒï¼UCä¼¯å…‹åˆ©å†å¼€æºç±»ChatGPTæ¨¡å‹ã€Œè€ƒæ‹‰ã€ï¼šæ•°æ®é‡å¤§æ²¡æœ‰ç”¨ï¼Œé«˜è´¨é‡æ‰æ˜¯ç‹é“https://zhuanlan.zhihu.com/p/621078208

ChatGPTå¹³æ›¿æ–¹æ¡ˆæ±‡æ€»https://zhuanlan.zhihu.com/p/618839784

å¾®è½¯å®£å¸ƒå¼€æº Deep Speed Chatï¼Œå¯å°†è®­ç»ƒé€Ÿåº¦æå‡ 15 å€ä»¥ä¸Šï¼Œå“ªäº›ä¿¡æ¯å€¼å¾—å…³æ³¨ï¼Ÿhttps://www.zhihu.com/question/595311294 

æ€»ç»“å½“ä¸‹å¯ç”¨çš„å¤§æ¨¡å‹LLMshttps://zhuanlan.zhihu.com/p/611403556

å¯èƒ½æ˜¯æœ€å…¨çš„å¼€æº LLM ï¼ˆå¤§è¯­è¨€æ¨¡å‹ï¼‰æ•´ç†

https://my.oschina.net/oscpyaqxylk/blog/8727824

[èµ„æºæ•´ç†]2023-05-11æ¯”è¾ƒå…¨çš„LLMsçš„èµ„æºæ•´ç† - è¿·é€”å°ä¹¦åƒ®çš„æ–‡ç«  - çŸ¥ä¹ https://zhuanlan.zhihu.com/p/628637821

ä¸­æ–‡å¼€æº1Bä»¥ä¸Šå¤§æ¨¡å‹æ±‡æ€» - nghuyongçš„æ–‡ç«  - çŸ¥ä¹ https://zhuanlan.zhihu.com/p/613239726

https://github.com/CLUEbenchmark/SuperCLUE

https://github.com/THUDM/ChatGLM-6B

ä¸€æ–‡æ±‡æ€»å¼€æºå¤§è¯­è¨€æ¨¡å‹ï¼Œäººäººéƒ½å¯ä»¥æ‹¥æœ‰è‡ªå·±çš„ChatGPT - æ— å¿Œçš„æ–‡ç«  - çŸ¥ä¹ https://zhuanlan.zhihu.com/p/622370602

æ–°å‘å¸ƒçš„ä¸€äº›å¼€æºå•†ç”¨æ¨¡å‹ - LokLokçš„æ–‡ç«  - çŸ¥ä¹ https://zhuanlan.zhihu.com/p/627785493

æœ€æ–°å‘å¸ƒï¼æˆªæ­¢ç›®å‰æœ€å¼ºå¤§çš„æœ€é«˜æ”¯æŒ65kè¾“å…¥çš„å¼€æºå¯å•†ç”¨AIå¤§æ¨¡å‹ï¼šMPT-7Bï¼ - æ•°æ®å­¦ä¹ çš„æ–‡ç«  - çŸ¥ä¹ https://zhuanlan.zhihu.com/p/627420365

å¤æ—¦å›¢é˜Ÿå¤§æ¨¡å‹ MOSS å¼€æºäº†ï¼Œæœ‰å“ªäº›æŠ€æœ¯äº®ç‚¹å€¼å¾—å…³æ³¨ï¼Ÿ - å­™å¤©ç¥¥çš„å›ç­” - çŸ¥ä¹ https://www.zhihu.com/question/596908242/answer/2994534005

æš´å‡»ä¸“å®¶æ¨¡å‹ï¼Metaæœ€æ–°å¤šæ¨¡æ€å¤§æ¨¡å‹ImageBindå·²å¼€æº - æ–°æ™ºå…ƒçš„æ–‡ç«  - çŸ¥ä¹ https://zhuanlan.zhihu.com/p/628370318

æ•´ç†å¼€æºå¯ç”¨çš„ä¸­æ–‡å¤§æ¨¡å‹LLMs - ç½—èƒ¤çš„æ–‡ç«  - çŸ¥ä¹ https://zhuanlan.zhihu.com/p/616812772

å¤§æ¨¡å‹çƒ­ç‚¹è®ºæ–‡ï¼šè°·æ­Œæ¨å‡º PaLM 2ã€Meta å¼€æº ImageBind - MegEngine Botçš„æ–‡ç«  - çŸ¥ä¹ https://zhuanlan.zhihu.com/p/628941792

å¦‚ä½•è¯„ä»·Googleæœ€æ–°å‘å¸ƒçš„PaLM2ï¼Œæ•ˆæœåè¶…GPT4ï¼Ÿ - ä¸€å †åºŸçº¸çš„å›ç­” - çŸ¥ä¹ https://www.zhihu.com/question/600311066/answer/3022625910

ä¸¤å¤§å¯å•†ç”¨å¼€æºå¤§æ¨¡å‹åŒæ—¶å‘å¸ƒï¼æ€§èƒ½ä¸è¾“LLaMAï¼Œç¾Šé©¼å®¶æ—åå­—éƒ½ä¸å¤Ÿç”¨äº† - é‡å­ä½çš„æ–‡ç«  - çŸ¥ä¹ https://zhuanlan.zhihu.com/p/627454901





## èµåŠ©æˆ‘ä»¬

æˆ‘ä»¬æ˜¯è°ï¼Ÿ

æˆ‘ä»¬æ˜¯ç¾¡é±¼æ™ºèƒ½ã€xianyu.aiã€‘ï¼Œä¸»è¦æˆå‘˜æ˜¯ä¸€ç¾¤æ¥è‡ªè€å’Œå±±ä¸‹ã€è¥¿æ¹–è¾¹ä¸Šçš„å’¸é±¼ä»¬ï¼Œå¡˜ä¸»å«ä½œç¾¡é±¼ï¼Œæƒ³åœ¨LLMsåšç‚¹æœ‰æ„ä¹‰çš„äº‹ï¼æˆ‘ä»¬çš„å£å·æ˜¯ï¼šåšOpenNLPå’ŒOpenXï¼å¸Œæœ›åœ¨OpenAIå·æ­»æˆ‘ä»¬ä¹‹å‰é€€å‡ºæ±Ÿæ¹–ï¼

ChatPiXiué¡¹ç›®ä¸ºç¾¡é±¼æ™ºèƒ½ã€xianyu.aiã€‘å‘èµ·çš„OpenNLPè®¡åˆ’çš„ç¬¬2ä¸ªæ­£å¼çš„å¼€æºé¡¹ç›®ï¼Œæ—¨åœ¨Open ChatGPT for everyoneï¼åœ¨ä»¥ChatGPT/GPT4ä¸ºä»£è¡¨çš„LLMsæ—¶ä»£ï¼Œåœ¨è¢«OpenAIå·æ­»ä¹‹å‰ï¼Œåšä¸€ç‚¹æœ‰æ„ä¹‰çš„äº‹æƒ…ï¼æœªæ¥æœ‰ä¸€å¤©ï¼Œç­‰åˆ°GPT-Xå‘å¸ƒçš„æ—¶å€™ï¼Œæˆ–è®¸æœ‰äººä¼šè¯´NLPä¸å­˜åœ¨äº†ï¼Œä½†æ˜¯æˆ‘ä»¬æƒ³è¯æ˜æœ‰äººæ›¾æ¥è¿‡ï¼

æœ¬é¡¹ç›®ç¬¬ä¸€ç‰ˆç”±æœ¬ç¾¡é±¼åˆ©ç”¨ä¸šåŠ¡æ—¶é—´ï¼ˆç†¬å¤œï¼‰ç‹¬ç«‹å®Œæˆï¼Œå—é™äºç²¾åŠ›å’Œç®—åŠ›ï¼Œæ‹–å»¶è‡³ä»Šï¼Œå¥½åœ¨é¡ºåˆ©å®Œæˆäº†ã€‚å¦‚æœå¤§å®¶è§‰å¾—æœ¬é¡¹ç›®å¯¹ä½ çš„NLPå­¦ä¹ /ç ”ç©¶/å·¥ä½œæœ‰æ‰€å¸®åŠ©çš„è¯ï¼Œæ±‚ä¸€ä¸ªå…è´¹çš„star! å¯Œå“¥å¯Œå§ä»¬å¯ä»¥è€ƒè™‘èµåŠ©ä¸€ä¸‹ï¼å°¤å…¶æ˜¯ç®—åŠ›ï¼Œ**ç§Ÿå¡çš„è´¹ç”¨å·²ç»è®©æœ¬ä¸å¯Œè£•çš„é±¼å¡˜å¿«è¦æ— é±¼å¯æ‘¸äº†**ï¼

<img src="https://xianyunlp.oss-cn-hangzhou.aliyuncs.com/uPic/image-20230324010955205.png" alt="image-20230324010955205" style="zoom: 25%;" />

## Starchart

[![Star History Chart](https://api.star-history.com/svg?repos=catqaq/ChatPiXiu&type=Date)](https://star-history.com/#catqaq/ChatPiXiu&Date)

## Contributors

<a href="https://github.com/catqaq/ChatPiXiu/graphs/contributors">
  <img src="https://contrib.rocks/image?repo=catqaq/ChatPiXiu" />
</a>
